Selection Process Report
A Comprehensive Report of Solved Selection Process Tasks By: Hasan Abdelhady Presented to: Professor/Danilo Vasconcellos Vargas Abstract—This is a comprehensive report that presents solutions to some of the Selection Process Tasks, specifically Tasks B1, B2, B5 A1, A6, and A7, with a detailed explanation of how each task is approched, and clear visualizations of the results achieved 1. Introduction
In this report, I present my solutions to the following Selection Process tasks of joining the LIS Lab led by Professor. Danilo
Vasconcellos Vargas. The tasks discussed in this report are: • B1: Reconstructing images from noisy inputs using denoising autoencoders.
• B2: Training a neural network for image similarity.
• B5: Using Game Theory to analyze Generative Adversarial Networks (GANs)
• A1: Comparing two Evolutionary Algorithms.
• A6: Applying a large language model to evaluate papers regarding both grammar and reasoning.
• A7: LLM Based Cognitive Graph Generation from Multiple Papers’ Processing Each problem is discussed in a separate section with 4 main subsections: • Problem Statement: Here I redefine the problem in more details based on how I understood it, just to paint a clear picture of how I saw the task while implementing the solution. • Methodoloy: Here I explain how I thought about solving each task and the steps taken to achieve the final results.
• Implementation: Here I share the main code snippets used to reach the final implementation of each task, and comment on the code used, and what purpose each line serves. • Results & Visualization: Here I share the results I got in the form of pictures, visualizations or just code printed in the terminal. 2. Problem [B1]: Image Reconstruction from Noisy Inputs 2.1. Problem Statement
Image reconstruction from noisy inputs is a fundamental challenge in computer vision. The goal is to recover the original
image from its noisy version. I approach this problem using a denoising autoencoder, which compresses the input image intoa
compressed representation via the encoder and then reconstructs the image using the decoder.
I’ll be applying this to both the MNIST and CIFAR-10 Datasets 2.2. Methodology
My approach to solving this task is divided into 3 stages: Creating a dataset of noist/clean images, Designing the autoencoder,
and training & since the MNIST and CIFAR-10 datasets differ in image resolution and channel configuration, I designed
separate autoencoder architectures for each. 1. Creating the Noisy dataset: • Dataset Loading: I use the built-in torchvision datasets for MNIST (28×28, grayscale) and CIFAR-10 (32×32, RGB).
• Noise Injection: I created custom dataset classes (e.g., NoisyMNIST and NoisyCIFAR10) that add Gaussian noise to the images. Each row of the dataset is a noisy image and the corresponding original image (without the noise). 2. Model Architecture: • For MNIST: The autoencoder is adapted for smaller 28×28 grayscale images. The encoder consists of two convolutional
layers with ReLU activations and max-pooling, which gradually reduce the spatial dimensions from 28×28 to 7×7 while
increasing the channel depth. Although there is no separately defined bottleneck block, the final encoder output of shape
(32, 7, 7) serves as the compressed representation. The decoder then uses transposed convolutions to upsample the image
back to 28×28, reconstructing a single-channel image. • For CIFAR-10: The autoencoder consists of an encoder that uses two convolutional blocks with batch normalization,
ReLU activations, and a max-pooling layer to reduce spatial resolution. A bottleneck block further compresses the
feature representation. The decoder mirrors the encoder by employing transposed convolutions (to upsample) and skip
connections that merge higher-resolution features from the encoder, ultimately reconstructing a 3-channel image. Selection Process Report Kyushu University April 30, 2025 1–55 Problem [B1]: Image Reconstruction from Noisy Inputs 3. Training Process: • Loss Function: I use the Mean Squared Error (MSE) loss to measure the difference between the reconstructed image and the original clean image. • Optimizer: The Adam optimizer is employed to adjust the network parameters.
• Data Loading: The datasets are loaded using PyTorch’s DataLoader, with a batch size of 128 and multiple worker processes to speed up data loading. 2.3. Implementation
The Python code for Task B1 has been split into several chunks. I’ll address what each chunk does below: 11 device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
12 print("Using device:", device) Code 1. Importing modules & checking device In the code above, we’re importing the needed modules ( Using Pytorch) & checking our currently used devices (CPU or GPU) to make sure we’re using the GPU for faster processing Noisy Dataset Class: 1 class NoisyMNIST(datasets.MNIST): def __init__(self, noise_factor=0.1, train=True, transform=None, target_transform=None, download=False): super().__init__(root=’./data’, train=train, transform=transform, target_transform=target_transform, download=download) self.noise_factor = noise_factor def __getitem__(self, index): img, target = super(NoisyMNIST, self).__getitem__(index)
# Create a noisy image by adding Gaussian noise
noise = self.noise_factor * torch.randn(img.size())
noisy_img = img + noise noisy_img = torch.clamp(noisy_img, 0., 1.) return noisy_img, img2 34 5 67 89 10 11 12 13 14 15 16
17 transform = transforms.Compose([
transforms.ToTensor(), 18
19 ]) In the code above, we define the NoisyMNIST Class which as the name suggests is just a noisy version of the MNIST that results in images like this: Code 2. Noisy MNIST Class Definition 2.3.1. MNIST Importing modules / Checking Device 1 import torch
2 import torch.nn as nn
3 import torch.optim as optim
4 from torch.utils.data import DataLoader
5 import torchvision
6 from torchvision import transforms, datasets
7 import matplotlib.pyplot as plt
8 import numpy as np
9 import torch.nn.functional asF 10 mnist_train_dataset = NoisyMNIST(noise_factor=noise_factor, train=True, transform=transform, download=True) 23 4 mnist_test_dataset = NoisyMNIST(noise_factor=noise_factor, train=False, transform=transform, download=True) These are going to be the images I’ll be trying to denoise (Reconstruct the original data from). Figure 1. Noisy images with a 30% noise_factor Initializing noise / splitting the dataset: 1 noise_factor = 0.3 Problem [B1]: Image Reconstruction from Noisy Inputs mnist_train_loader = DataLoader(mnist_train_dataset, batch_size=batch_size, shuffle=True)
= DataLoader(mnist_test_dataset, batch_size=batch_size, shuffle=False) 9 mnist_test_loader batch_size = 128 56 78 Code 3. Data Splitting & Noise Initialization In this part of the code, we’re setting our noise_factor, which we can change to any value we like, I chose 0.3 just to make ita little challenging for our Auto Encoder.
After that, we can see the mnist_train_dataset and the mnist_test_dataset variables, which represent our training and testing
data; they’re split using the built-in "train" parameter in the datasets.MNIST module, which when set to "True" assigns some
portion of the data as "Training Data" to the object made from the class, and the same goes for testing when "train" is set to "False". After that, we assign our batch_size to 128 to make sure our dataset is split into a reasonable number of batches during both training and evaluation. Then our mnist_train_loader & mnist_test_loader variables, which are simply how we load our data using the DataLoader class from Pytorch so that we can pass this data to our Model. The MNIST Autoencoder: 1 class MNISTAutoencoder(nn.Module): def __init__(self): super(MNISTAutoencoder, self).__init__()
self.encoder = nn.Sequential( nn.Conv2d(1, 16, kernel_size=3, padding=1),
nn.ReLU(),
nn.MaxPool2d(2, 2),
# (16,14,14)
nn.Conv2d(16, 32, kernel_size=3, padding=1), # (32,14,14)
nn.ReLU(),
nn.MaxPool2d(2, 2) # (16,28,28) # (32,7,7))
self.decoder = nn.Sequential( nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2), # (16,14,14)
nn.ReLU(),
nn.ConvTranspose2d(16, 1, kernel_size=2, stride=2),
nn.Sigmoid() # (1,28,28)) 23 45 67 89 10 11 12 13 14 15 16 17 18 Problem [B1]: Image Reconstruction from Noisy Inputs 19 20 21 22 def forward(self, output): output = self.encoder(output)
output = self.decoder(output)
return output Code 4. Evaluation and Visualization of Denoising Results In this part, I’m defining my Autoencoder, which serves as the "Reconstructor" of the images. It is composed of two main
components: an Encoder and a Decoder. The encoder compresses the input image by applying successive convolutional
filters, non-linear activations, and pooling operations, thereby reducing its spatial dimensions while retaining essential features.
The decoder then upsamples this compressed representation using transposed convolutions, gradually reconstructing the image
to its original dimensions. Skip connections (when used) allow the decoder to access higher-resolution feature maps from the
encoder to improve the reconstruction quality. And now let’s take a look at a more visual representation of this architecture! Input
28 × 28 × 1 28 × 28 × 16 Conv2d
3 × 3
16 filters
padding=1 Extracts features from input Output
28 × 28 × 1 Sigmoid 28 × 28 × 16 14 × 14 × 16 Encoder ReLU Decoder ConvTranspose2d
2 × 2
16 filter
stride=2 MaxPool2d
2 × 2 Downsamples
by factor of2 ReLU 28 × 28 × 1 Normalizes to [0,1] 28 × 28 × 1 7 × 7 × 32 14 × 14 × 32 Conv2d
3 × 3
32 filters
padding=1 ConvTranspose2d
2 × 2
16 filters
stride=2 14 × 14 × 16 Upsamples by
factor of2 14 × 14 × 32 7 × 7 × 32 ReLU MaxPool2d
2 × 2 Latent
7 × 7 × 32 Figure 2. The MNIST Autoencoder architecture (3D) Model Initialization and Hyperparameter Setup: 1 mnist_model = MNISTAutoencoder().to(device)
2 criterion = nn.MSELoss()
3 optimizer = torch.optim.Adam(mnist_model.parameters(), lr=1e-3) Code 5. Model Initialization, Loss Function and Optimizer Setup In this snippet, I instantiate the MNIST autoencoder model and transfer it to the appropriate device (GPU or CPU) using the.to(device) method. Next, I set up the Mean Squared Error (MSE) loss function, which measures the pixel-wise difference
between the reconstructed image and the original clean image. I also define the optimizer as Adam, which adjusts the model’s
parameters based on the computed gradients. These settings form the basis of the training process by specifying how the model
should learn from the data. Training Loop: 1 num_epochs = 10
2 for epoch in range(num_epochs):
mnist_model.train()
running_loss = 0.0
for noisy_imgs, clean_imgs in mnist_train_loader: 34 56 78 noisy_imgs = noisy_imgs.to(device)
clean_imgs = clean_imgs.to(device) • Setting the model to training mode with mnist_model.train().
• Iterating over the training data in batches using the mnist_train_loader.
• Moving each batch to the selected device.
• Forward propagating the noisy images through the autoencoder to obtain reconstructed outputs.
• Computing the loss by comparing the reconstructed outputs with the original images using the MSE loss.
• Zeroing the gradients with optimizer.zero_grad(), backpropagating the loss with loss.backward(), and updating the model parameters with optimizer.step(). • Accumulating and printing the average loss per epoch. This loop is executed over 10 epochs, which I found was a good-enough number of epochs for this current application, allowing
the model to iteratively refine its ability to reconstruct clean images from noisy inputs. Code 7. Evaluation and Visualization Function for Denoising Results This portion of the code defines a function show_images() that is used to visualize the denoising results. The function performs the following actions: • It converts the noisy input, the autoencoder output (reconstructed image), and the original image from PyTorch tensors to NumPy arrays. • It creates a figure with 3 rows and 10 columns using plt.subplots(), where each row corresponds to a type of image: noisy = noisy.cpu().numpy()
output = output.cpu().detach().numpy()
original = original.cpu().numpy() fig, axes = plt.subplots(3, 10, figsize=(15, 5)) # Row titles
row_titles = [’Noisy’, ’Reconstructed’, ’Original’] for row_idx, images in enumerate([noisy, output, original]): for col_idx in range(10): axes[row_idx][col_idx].imshow(images[col_idx].squeeze(), cmap=’gray’) axes[row_idx][0].set_ylabel(row_titles[row_idx], fontsize=14) plt.tight_layout()
plt.show() 23 4 56 78 9 10 11 12 13 14 15 16 17 18 19 Evaluation and Visualization: 1 def show_images(noisy, output, original): 20
21 # Run this after evaluation
22 mnist_model.eval()
23 with torch.no_grad(): 24 25 26 27 28 for noisy_imgs, clean_imgs in mnist_test_loader: noisy_imgs = noisy_imgs.to(device)
outputs = mnist_model(noisy_imgs)
show_images(noisy_imgs, outputs, clean_imgs)
break Problem [B1]: Image Reconstruction from Noisy Inputs9 10 11 12 13 14 15 16 17 18 outputs = mnist_model(noisy_imgs)
loss = criterion(outputs, clean_imgs) optimizer.zero_grad()
loss.backward()
optimizer.step() running_loss += loss.item() print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(mnist_train_loader):.4f}") Code 6. Training Loop for the MNIST Autoencoder This section implements the training loop for the MNIST autoencoder. The key steps include: Problem [B1]: Image Reconstruction from Noisy Inputs – The top row displays the noisy images.
– The middle row displays the reconstructed (denoised) images.
– The bottom row displays the original clean images. • It sets a y-axis label for each row (“Noisy”, “Reconstructed”, and “Original”) for clarity.
• Finally, it uses plt.tight_layout() to ensure the subplots are well arranged and plt.show() to display the figure. At the end of this file, I set the autoencoder in evaluation mode (using mnist_model.eval()) and pass a batch from the
mnist_test_loader through the model. The resulting outputs, along with the corresponding noisy and original images, are
then displayed using the show_images() function.
And this is our final Output: Figure 3. Reconstruction results: noisy inputs, model outputs, and original images. def __init__(self, noise_factor=0.1, train=True, transform=None, target_transform=None, download=False): super().__init__(root=’./data’, train=train, transform=transform, target_transform=target_transform, download=download) self.noise_factor = noise_factor def __getitem__(self, index): # Get the clean image and label from CIFAR-10
img, target = super(NoisyCIFAR10, self).__getitem__(index)
# Create a noisy image by adding Gaussian noise
noise = self.noise_factor * torch.randn(img.size())
noisy_img = img + noise
# Clamp the values to [0, 1]
noisy_img = torch.clamp(noisy_img, 0., 1.)
# Return noisy image as input and clean image as target
return noisy_img, img2 34 5 67 89 10 11 12 13 14 15 16 17
18 # Define our transform ( Image to Tensor)
19 transform = transforms.Compose([
transforms.ToTensor(), 20
21 ]) Code 8. Noisy Version of the CIFAR-10 Dataset + Transform Creation In the code above, we define the NoisyCIFAR-10 Class which is just a noisy version of the CIFAR-10 similar to the MNIST one, that results in images like this: Problem [B1]: Image Reconstruction from Noisy Inputs 2.3.2. CIFAR-10
Skip Imports ( Same as above) Defining the NoisyCIFAR10 Class: 1 class NoisyCIFAR10(datasets.CIFAR10): Adding Noise & splitting the dataset: Figure 4. Noisy images with a 0.1 noise_factor 1 #Setting up my noise_factor and loading the training/testing data using CIFAR-10’s Default split
2 #train=True loads 50k images as training data, and train=False loads 10k imaegs as testing data
3 noise_factor = 0.1
4 cifar_train_dataset = NoisyCIFAR10(noise_factor=noise_factor, train=True, transform=transform, download=True = NoisyCIFAR10(noise_factor=noise_factor, train=False, transform=transform, download= 5 cifar_test_dataset
↪ True) batch_size = 128 67 ↪) 8 cifar_train_loader = DataLoader(cifar_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
= DataLoader(cifar_test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
9 cifar_test_loader Code 9. Declaring our noise value + splitting the dataset Similarily to the MNIST code, we’re setting our noise_factor, which we can change to any value we like. I chose 0.1. We can also see the cifar_train_dataset and the cifar_test_dataset variables, which represent our training and testing data; they’re split also in the same way the MNIST dataset was split, via the "train" parameter Problem [B1]: Image Reconstruction from Noisy Inputs After that, we assign our batch_size to 128 to make sure our dataset is split into a reasonable number of batches during both training and evaluation. The CIFAR-10 Autoencoder: 1 class DenoisingAutoencoder(nn.Module): def __init__(self): super(DenoisingAutoencoder, self).__init__() # Encoder: Convolution layers with BatchNorm
self.enc_conv1 = nn.Sequential( nn.Conv2d(3, 32, kernel_size=3, padding=1),
nn.ReLU(inplace=True) # (32, 32, 32))
self.enc_conv2 = nn.Sequential( nn.Conv2d(32, 64, kernel_size=3, padding=1), # (64, 32, 32)
nn.ReLU(inplace=True)) self.pool = nn.MaxPool2d(2, 2) # reduces spatial dimensions by half # Bottom (bottleneck)
self.bottleneck = nn.Sequential( nn.Conv2d(64, 128, kernel_size=3, padding=1), # (128, 16, 16)
nn.ReLU(inplace=True)) # Decoder: using transposed convolutions with skip connections
self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)
self.dec_conv1 = nn.Sequential( # (64, 32, 32) nn.Conv2d(64 + 64, 64, kernel_size=3, padding=1), # I’ll be combining skip connection (enc2 + ↪ up1, hence the 64+64) nn.ReLU(inplace=True)) self.final = nn.Sequential( nn.Conv2d(64, 3, kernel_size=3, padding=1),
nn.Sigmoid()) def forward(self, x): # Encoder
enc1 = self.enc_conv1(x)
enc2 = self.enc_conv2(enc1)
pooled = self.pool(enc2)
# Bottleneck
bottleneck = self.bottleneck(pooled) # (32,32,32) # (64,32,32)
# (64,16,16) # (128,16,16) # Decoder
up1 = self.up1(bottleneck)
# (64,32,32)
# Skip connection: concatenate with enc2
dec1 = torch.cat([up1, enc2], dim=1)
dec1 = self.dec_conv1(dec1) # (64+64,32,32) = (128,32,32)
# (64,32,32) output = self.final(dec1)
return output # (3,32,32) Code 10. Declaring our noise value + splitting the dataset2 3 45 67 89 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Problem [B1]: Image Reconstruction from Noisy Inputs The autoencoder code presented above is designed to remove noise from RGB images (e.g. CIFAR-10) by learning to
reconstruct a clean version from its noisy input. The network is divided into three primary stages: the encoder, the bottleneck,
and the decoder.
Encoder: • First Convolution Block (enc_conv1): This block begins with a 2D convolution layer that transforms the input image
from 3 channels to 32 channels using 3 × 3 filters with a padding of 1. The padding ensures that the spatial dimensions
remain 32 × 32. One Batch Normalization layer normalizes the activations to stabilize and accelerate training. Finally, a
ReLU activation applies non-linearity by setting negative values to zero, which allows the network to learn more complex
functions. • Second Convolution Block (enc_conv2): The output from the first block is further processed by a second convolution
layer that increases the depth from 32 to 64 channels, again using 3 × 3 filters with padding 1. A Batch Normalization
layer and a ReLU activation follow, ensuring that the features are normalized and non-linearity is introduced. • Pooling: After these convolutions, a MaxPooling layer with a kernel size of 2 and stride of 2 halves the spatial dimensions (from 32 × 32 to 16 × 16), thereby concentrating the essential features and reducing computational cost. Bottleneck: • The bottleneck consists of a convolution layer that increases the number of channels from 64 to 128 while maintaining the
spatial dimensions at 16 × 16 (using a 3 × 3 kernel with padding 1). This stage creates a compact, high-level representation
of the input image. Batch Normalization and ReLU are applied to this block as well. Decoder: • Upsampling (up1): The decoder begins with a transposed convolution (also known as a deconvolution) that upsamples
the feature maps from the bottleneck from 16 × 16 back to 32 × 32, while reducing the channel depth from 128 to 64.
• Skip Connection and Refinement (dec_conv1): A skip connection concatenates the upsampled feature maps with
the output of the encoder’s second convolution block (enc_conv2). This concatenation effectively doubles the number
of channels from 64 to 128 and ensures that high-resolution details from the encoder are directly available during
reconstruction. The concatenated features are then processed by another convolution layer (with a 3 × 3 filter and padding
1), Batch Normalization, and ReLU activation, bringing the channel depth back to 64. • Final Reconstruction (final): The last layer of the network is a convolution layer that maps the 64 feature channels to
3 output channels (the RGB components). A Sigmoid activation function is applied to ensure that the output pixel values
are confined within the range [0, 1], which is suitable for image data. Skip Connections: Skip connections (implemented via the concatenation of up1 with enc_conv2) play a critical role by
preserving high-resolution information from earlier layers. This mechanism helps the decoder produce more detailed and
accurate reconstructions by compensating for the information loss during pooling. Overall, the autoencoder compresses the noisy input image into a low-dimensional representation and then reconstructs it
back to its original form. The network leverages convolutional layers for feature extraction, normalization for stable learning,
non-linear activations for complex mappings, and transposed convolutions for upsampling, resulting in an effective denoising
architecture. Input
3 × 32 × 32 32 × 32 × 32 Conv2d
3 × 3
32 filters
padding=1 enc_conv1 => 32 × 32 × 32 32 × 32 × 32 BatchNorm2d ReLU Encoder 64 × 32 × 32 Conv2d
3 × 3
64 filters
padding=1 enc_conv2 => 64 × 32 × 32 64 × 32 × 32 64 × 16 × 16 BatchNorm2d ReLU MaxPool2d
2 × 2 Conv2d
3 × 3
128 filters
padding=1 128 × 16 × 16 Conv2d
3 × 3
64 filters
padding=1 64 × 32 × 32 dec_conv1 Bottleneck 128 × 16 × 16 128 × 16 × 16 BatchNorm2d ReLU Concat
Skip Connection 128 × 32 × 32 torch.cat ConvTranspose2d
2 × 2
64 filters
stride=2 64 × 32 × 32 up1 Output
3 × 32 × 32 Sigmoid 3 × 32 × 32 Conv2d
3 × 3
3 filters
padding=1 3 × 32 × 32 final Decoder ReLU BatchNorm2d 64 × 32 × 32 64 × 32 × 32 Figure 5. The CIFAR-10 Autoencoder Architecture with Skip Connections (3D) Problem [B1]: Image Reconstruction from Noisy Inputs Model Initialization and Hyperparameter Setup: 1 # Instantiate the model, loss function and optimizer
2 cifar_model = DenoisingAutoencoder().to(device)
3 criterion = nn.MSELoss()
4 optimizer = optim.Adam(cifar_model.parameters(), lr=1e-3) # Mean Squared Error loss between clean and reconstructed image Code 11. Noisy Version of the CIFAR-10 Dataset + Transform Creation In this snippet, I instantiate the CIFAR-10 autoencoder model and transfer it to the appropriate device (GPU or CPU) using
the.to(device) method. Next, I set up the Mean Squared Error (MSE) loss function and then also defined our Adam Optimizer.
These settings form the basis of the training process by specifying how the model should learn from the data. Training Loop: 1 # Training loop
2 num_epochs = 30
3 print("Starting training...")
4 for epoch in range(num_epochs):
cifar_model.train()
running_loss = 0.0
for noisy_imgs, clean_imgs in cifar_train_loader: 67 5 noisy_imgs = noisy_imgs.to(device)
clean_imgs = clean_imgs.to(device) optimizer.zero_grad()
outputs = cifar_model(noisy_imgs) loss = criterion(outputs, clean_imgs)
loss.backward()
optimizer.step() running_loss += loss.item() * noisy_imgs.size(0) epoch_loss = running_loss / len(cifar_train_loader.dataset)
print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}") 89 10 11 12 13 14 15 16 17 18 19 20 21 22 23 print("Training completed.") Code 12. Training the cifar_model with 30 epochs The training loop for the CIFAR-10 denoising autoencoder is executed for 30 epochs. In each epoch, the following steps are performed: • Training Mode: The model is set to training mode by calling cifar_model.train(). This ensures that layers such as dropout and batch normalization behave appropriately during training. • Iterating Over Batches: The training data is loaded in batches via the cifar_train_loader. For each batch, a pair of
tensors is returned: noisy_imgs (the input images with added noise) and clean_imgs (the corresponding clean target
images). • Data Transfer: Both the noisy and clean images are moved to the selected device (GPU or CPU) to ensure fast computation.
• Forward Pass: The noisy images are fed into the autoencoder (cifar_model) to obtain the reconstructed (denoised) outputs. • Loss Calculation: The Mean Squared Error (MSE) loss is computed between the reconstructed outputs and the clean
images. The MSE loss measures the average squared difference per pixel and is well-suited for image reconstruction tasks. • Backpropagation and Optimization: Before computing the gradients, the gradients are zeroed with optimizer.zero_grad(). Then, loss.backward() is called to perform backpropagation, and the optimizer updates the model’s parameters with
optimizer.step(). • Loss Accumulation and Reporting: The loss for each batch, scaled by the batch size, is accumulated to compute the average loss for the epoch. After all batches have been processed, the epoch’s average loss is printed. This structured loop allows the model to gradually minimize the reconstruction error over multiple epochs, leading to better
denoising performance. At the conclusion of the loop, the training is declared complete. This iterative process is crucial for
enabling the autoencoder to learn the mapping from noisy inputs to clean outputs. 1 # Evaluate and visualize some denoising results from the test set
2 cifar_model.eval()
3 dataiter = iter(cifar_test_loader)
4 noisy_imgs, original_imgs = next(dataiter) # ’original_imgs’ are the clean, original images
5 noisy_imgs = noisy_imgs.to(device)
6 with torch.no_grad(): outputs = cifar_model(noisy_imgs) # Convert tensors to numpy arrays for plotting (move to CPU and detach) 7 89 Problem [B1]: Image Reconstruction from Noisy Inputs Evaluation and Visualization of Reconstruction Results: 10 noisy_imgs = noisy_imgs.cpu().numpy()
11 outputs = outputs.cpu().numpy()
12 original_imgs = original_imgs.cpu().numpy() 13
14 # Plot noisy, denoised, and original images for a few examples
15 n = 15
16 fig, axes = plt.subplots(3, n, figsize=(20, 6))
17 row_titles = [’Noisy’, ’Reconstructed’, ’Original’] 18 19 for row in range(3): 20 21 22 23 24 25 26 27 28 for i in range(n):
if row == 0: axes[row, i].imshow(np.transpose(noisy_imgs[i], (1, 2, 0))) elif row == 1: axes[row, i].imshow(np.transpose(outputs[i], (1, 2, 0))) elif row == 2: axes[row, i].imshow(np.transpose(original_imgs[i], (1, 2, 0))) axes[row, i].set_xticks([])
axes[row, i].set_yticks([]) 29
30 # Set a vertical label for each row on the left-most subplot
31 axes[0, 0].set_ylabel(row_titles[0], fontsize=14, rotation=90, labelpad=10)
32 axes[1, 0].set_ylabel(row_titles[1], fontsize=14, rotation=90, labelpad=10)
33 axes[2, 0].set_ylabel(row_titles[2], fontsize=14, rotation=90, labelpad=10) 34
35 plt.tight_layout()
36 plt.show() Code 13. Training Loop & Loss calculation The code snippet above demonstrates how I evaluate the denoising autoencoder on the test set and visualize the results. The process is divided into several steps: 1. Evaluation Mode: The model is set to evaluation mode with cifar_model.eval(), which deactivates dropout and batch normalization updates during inference. 2. Loading a Batch: A batch of test images is obtained from cifar_test_loader using an iterator. The batch consists of
pairs, where noisy_imgs contains the noisy inputs and original_imgs contains the corresponding clean images.
3. Moving Data to the Device: The noisy images are moved to the same device as the model (GPU or CPU) using to(device) to ensure computation is performed on the correct hardware. 4. Forward Pass without Gradient Computation: Using a with torch.no_grad(): block prevents tracking of gradients
during inference. The model processes the noisy images to generate outputs (the reconstructed, denoised images).
5. Converting Tensors to NumPy Arrays: Both the input (noisy), the model outputs, and the original images are moved
back to the CPU and detached from the computation graph, then converted to NumPy arrays. This conversion is necessary
because the plotting library (Matplotlib) works with NumPy arrays. 6. Plotting the Results: I create a figure with three rows and a fixed number of columns (15 examples per row). Each row corresponds to: • Row 1: Noisy input images.
• Row 2: Reconstructed (denoised) images produced by the autoencoder.
• Row 3: Original, clean images. For each subplot, the images are displayed after transposing the NumPy array from the format (channels, height,
width) to (height, width, channels). Ticks on the axes are removed for clarity. 7. Adding Vertical Labels: Instead of labeling each individual image, vertical labels are added to the left-most subplot of Problem [B1]: Image Reconstruction from Noisy Inputs each row to denote the row’s category (“Noisy”, “Reconstructed”, and “Original”). The labels are rotated 90 degrees fora
vertical orientation and are positioned with appropriate padding. 8. Layout and Display: Finally, plt.tight_layout() is used to adjust the spacing between subplots, and plt.show() renders the figure. This systematic approach to evaluation not only confirms the qualitative performance of the denoising autoencoder but also
provides a clear, visual comparison between the noisy inputs, the reconstructed outputs, and the ground-truth clean images. Figure 6. Reconstruction results: noisy inputs, model outputs, and original images. Problem [B2]: Calculating Similarity Between Images Using a Neural Network 3. Problem [B2]: Calculating Similarity Between Images Using a Neural Network 3.1. Problem Statement
The goal of this problem is to train a neural network that can calculate the similarity between pairs of images. The network
should be able to distinguish whether two images belong to the same class or not, using the MNIST dataset. 3.2. Methodology
My approach is to use a Siamese neural network architecture that learns to embed images into a feature space where similar
images are close together and dissimilar images are far apart.
I train the network using pairs of images and a binary label indicating whether the images are from the same class (similar) or
not (dissimilar). The cosine similarity between the embeddings is used as the similarity metric, and the network is trained with
a binary cross-entropy loss. 3.3. Implementation
The implementation is divided into several components, each explained below. 1. Data Preparation and Loading 1 import torch
2 import torch.nn as nn
3 import torch.optim as optim
4 from torch.utils.data import Dataset, DataLoader
5 from torchvision.datasets import MNIST
6 import torchvision.transforms asT
7 import random
8 import torch.nn.functional asF
9 from collections import defaultdict
10 import matplotlib.pyplot as plt
11 import numpy as np
12 from matplotlib.patches import Rectangle
13 import pandas as pd
14 from torchvision.utils import make_grid
15 import torchvision.transforms as transforms 16
17 transform = T.Compose([
T.ToTensor(),
T.Normalize(mean=0.1307, std=0.3081) 18 19
20 ]) 21
22 # Loading MNIST and applying transforms to the data
23 original_dataset = MNIST(root=’./data’, train=False, download=False, transform=None) #we’ll use it later for ↪ visualizations 24 train_dataset = MNIST(root=’./data’, train=True, download=True, transform=transform)
25 test_dataset = MNIST(root=’./data’, train=False, download=True, transform=transform) Code 14. Data loading and preprocessing Here I import all of the libraries I’ll be using throughout this implementation in addition to loading and preprocessing the
MNIST dataset, preparing it for use in the similarity model.
I define a transform that converts my dataset to tensors & adds normalization to it via T.Normalize(mean=0.1307, std=0.3081)
‘these are the verified numbers for the MNIST Dataset to make the data have a zero-mean with unit-variance, to make it easier for
the network to learn. def __init__(self, dataset, num_pairs=60000): self.dataset
self.num_pairs = dataset
= num_pairs # Build a map of all the labels & corresponding indices
self.labels_hashmap = defaultdict(list)
for idx, (_, lbl) in enumerate(dataset):
self.labels_hashmap[lbl].append(idx) def __len__(self): return self.num_pairs def __getitem__(self, _): # Pick first image at random
i1
img1, l1 = self.dataset[i1] = random.randrange(len(self.dataset)) # With probability 0.5 pick same-label, else different-label
if random.random() < 0.5: i2 = random.choice(self.labels_hashmap[l1])
target = 1.0 else: # choose a label!= l1
l2 = random.choice([lbl for lbl in self.labels_hashmap if lbl!= l1])
i2 = random.choice(self.labels_hashmap[l2])
target = 0.0 img2, _ = self.dataset[i2]
return img1, img2, torch.tensor(target, dtype=torch.float32) Code 15. PairDataset class for generating image pairs4 56 78 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Problem [B2]: Calculating Similarity Between Images Using a Neural Network # Pair Dataset with caching for efficiency 12 3 class PairDataset(Dataset): 2. Pair Dataset Construction What this does is simply creating a new dataset using the MNIST dataset using a custom pytorch class called PairDataset.
this new dataset has ‘pairs’ of images put together based on their classes, images that have the same class, for example, 2 images
of the number 3 are labeled as similar or 1 whereas images with different classes are labeled as dissimilar or 0 creatinga
dataset that looks like this: Figure 7. Pairs Dataset. Code explanation: • Constructor (__init__): – Accepts a labeled dataset and the number of pairs to generate.
– Builds a lookup dictionary by_label that maps each label to a list of dataset indices with that label. ex. Problem [B2]: Calculating Similarity Between Images Using a Neural Network1 self.labels_hashmap[3] = [7, 15, 42,...] – This enables efficient sampling of positive (same label) and negative (different label) pairs. • __len__ method: – Returns the total number of pairs the dataset should provide. This allows PyTorch’s data loader to know how many samples it can draw. • __getitem__ method: – This method is called a number of times equivalent to num_pairs
– It returns 2 images img1 & img2 with a label indicating if they’re similar (1) or not (0)
– Randomly selects the first image from the dataset.
– With a 50% chance, it selects the second image: * From the same label group to create a positive pair (label 1.0).
* From a different label group to create a negative pair (label 0.0). – Returns both images along with a binary label indicating whether they belong to the same class or not. 3. Embedding Network 1 # Simple embedding CNN
2 class EmbeddingNet(nn.Module):
def __init__(self): 34 56 78 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 super().__init__()
self.conv = nn.Sequential( nn.Conv2d(1, 32, kernel_size=5, padding=2),
nn.BatchNorm2d(32),
nn.ReLU(),
nn.MaxPool2d(2), # Add batch normalization nn.Conv2d(32, 64, kernel_size=5, padding=2),
nn.BatchNorm2d(64),
nn.ReLU(),
nn.MaxPool2d(2), # Add batch normalization) self.fc = nn.Sequential( nn.Flatten(),
nn.Linear(64 * 7 * 7, 128),
nn.BatchNorm1d(128),
nn.ReLU(), # Add batch normalization) def forward(self, x):
x = self.conv(x)
x = self.fc(x)
# L2 normalize embeddings
x = F.normalize(x, p=2, dim=1)
returnx Code 16. Embedding network for feature extraction This class implements a small convolutional neural network that maps each input image into a 128-dimensional embedding
vector on the unit hypersphere. It consists of two convolutional blocks (with BatchNorm, ReLU and pooling), followed bya
fully–connected projection and an L2-normalization step. Code explanation: • Constructor (__init__): – self.conv: a nn.Sequential of two convolutional blocks: * nn.Conv2d(1, 32, kernel_size=5, padding=2) learns 32 filters while preserving the 28×28 spatial size
* nn.BatchNorm2d(32) normalizes each of the 32 feature–map channels per mini-batch Problem [B2]: Calculating Similarity Between Images Using a Neural Network * nn.ReLU() introduces non-linearity
* nn.MaxPool2d(2) downsamples spatial dimensions by a factor of2
* The second block repeats the pattern with 32 → 64 channels – self.fc: a nn.Sequential that produces a 128-dim vector: * nn.Flatten() collapses the final 64×7×7 feature maps into a single vector
* nn.Linear(64*7*7, 128) projects to a 128-dim embedding
* nn.BatchNorm1d(128) standardizes the embedding activations per batch
* nn.ReLU() adds non-linearity before normalization • Forward pass (forward): – Applies the two conv blocks: x = self.conv(x)
– Maps to embedding space: x = self.fc(x)
– Performs L2 normalization: x = F.normalize(x, p=2, dim=1), so that each embedding vector satisfies ‖𝑥‖2 = 1, making cosine similarity simply a dot product. cosine_sim(𝑥1, 𝑥2) = 𝑥1 ⋅ 𝑥2
‖𝑥1‖2 ‖𝑥2‖2 Since the embeddings are L2-normalized, this reduces to a simple dot product: cosine_sim(𝑥1, 𝑥2) = 𝑥1 ⋅ 𝑥2 – Returns the normalized embeddings This architecture • Extracts hierarchical spatial features via convolution and pooling
• Uses BatchNorm to stabilize and accelerate training
• Produces unit–length embeddings suitable for cosine-based similarity EmbeddingNet Architecture 28 × 28 × 32 14 × 14 × 32 14 × 14 × 64 Input
28 × 28 × 1 Conv2d
(1→32), 5×5, pad=2 BatchNorm2d ReLU MaxPool2d
2×2 Conv2d
(32→64), 5×5, pad=2 BatchNorm2d ReLU 7 × 7 × 64 MaxPool2d
2×2 L2 Normalize
‖𝑥‖2 = 1 128 ReLU BatchNorm1d Linear
(3136→128) Flatten
7 × 7 × 64 128 3136 Figure 8. The Embedding Net Architecture Figure 9. 3D View Here we define a model that takes two images, computes their embeddings as single vectors of 128 dimensions, and outputs Code 17. Similarity model using cosine similarity a similarity score using cosine similarity. Code explanation: • class SimilarityModel(nn.Module): Inherits from PyTorch’s base module to build a custom network that compares two embeddings. • __init__(self, embedding_net): – Stores the provided embedding_net (an instance of EmbeddingNet) for feature extraction.
– Defines self.projection = nn.Linear(1,1) — a learnable linear layer that takes a single scalar (raw cosine) and outputs an unbounded logit for binary classification. • forward(self, img1, img2): – emb1 = self.embedding_net(img1) — computes a [𝐵 × 128] embedding for the first image.
– emb2 = self.embedding_net(img2) — computes a [𝐵 × 128] embedding for the second image.
– sim = F.cosine_similarity(emb1, emb2) — returns a [𝐵] tensor of cosine similarities in [−1, 1].
– sim = sim.unsqueeze(1) — reshapes from [𝐵] to [𝐵 × 1] so it can be fed into the linear projection.
– logits = self.projection(sim) — applies the learnable shift/scale to produce raw logits [𝐵 × 1].
– return logits.squeeze(1) — removes the singleton dimension, yielding a [𝐵] tensor of similarity logits. • get_raw_similarity(self, img1, img2): Computes and returns the unprojected cosine similarity ∈ [−1, 1], bypassing the projection head—useful for analysis or threshold-based comparisons. def __init__(self, embedding_net): super().__init__()
self.embedding_net = embedding_net
# Add a projection layer to map similarity to proper range for BCE
# Input: [B, 1] => Output: [B, 1] (B = Batch Size)
self.projection = nn.Linear(1, 1) def forward(self, img1, img2): # img1, img2: [B, 1, 28, 28] (e.g., MNIST images) # emb1, emb2: [B, 128] (128-dim embedding from embedding_net)
emb1 = self.embedding_net(img1)
emb2 = self.embedding_net(img2) # sim: [B] (cosine similarity per pair, range [-1, 1])
sim = F.cosine_similarity(emb1, emb2) # sim.unsqueeze(1): [B, 1] to match projection layer input
# self.projection(sim): [B, 1]
# squeeze(1): back to [B] for BCE loss compatibility
return self.projection(sim.unsqueeze(1)).squeeze(1) def get_raw_similarity(self, img1, img2): """Get raw cosine similarity without projection"""
# emb1, emb2: [B, 128]
emb1 = self.embedding_net(img1)
emb2 = self.embedding_net(img2) # Returns cosine similarity in range [-1, 1]
return F.cosine_similarity(emb1, emb2) 23 45 67 89 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 4. Similarity Model 1 class SimilarityModel(nn.Module): Problem [B2]: Calculating Similarity Between Images Using a Neural Network Figure 10. Visual Representation of the Similarity Model 5. Training Setup and Evaluation 1 # Helper function for evaluation
2 def evaluate(model, data_loader, device, criterion): model.eval()
total_loss = 0
correct = 0
total = 0 with torch.no_grad(): for img1, img2, labels in data_loader: img1 = img1.to(device)
img2 = img2.to(device)
labels = labels.to(device) outputs = model(img1, img2)
loss = criterion(outputs, labels) total_loss += loss.item() predicted = (torch.sigmoid(outputs) > 0.5).float()
correct += (predicted == labels).sum().item()
total += labels.size(0) accuracy = correct / total
avg_loss = total_loss / len(data_loader)
return avg_loss, accuracy3 45 6 78 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26
27 # Use mixed precision for faster training on compatible GPUs
28 scaler = torch.amp.GradScaler() if torch.cuda.is_available() else None 29
30 # Setup device with proper synchronization
31 device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
32 if device.type == ’cuda’: 33 34 35 # Set these for consistent behavior across different machines
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False 36
37 # Initialize model, loss and optimizer
38 model = SimilarityModel(EmbeddingNet()).to(device)
39 criterion = nn.BCEWithLogitsLoss()
40 optimizer = optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-5)
41 scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5) Code 18. Training setup and evaluation utilities Here I set up the training environment and define some evaluation utilities, starting with the evaluate function which Problem [B2]: Calculating Similarity Between Images Using a Neural Network Problem [B2]: Calculating Similarity Between Images Using a Neural Network returns the average loss and accuracy for a given model, and moving on to defining our model, criterion, optimizer, and
scheduler. Code explanation: • model.eval(): puts the network into evaluation mode, disabling dropout and BatchNorm updates.
• with torch.no_grad(): disables gradient computation to save memory and speed up inference.
• Inside the loop, each batch of image pairs and labels is moved to device (CPU or GPU).
• outputs = model(img1, img2) computes the raw logits for each pair.
• loss = criterion(outputs, labels) computes the binary cross-entropy loss with logits.
• We accumulate total_loss via loss.item().
• predicted = (torch.sigmoid(outputs) > 0.5).float() thresholds the sigmoided logits at 0.5 to get binary predictions. • correct += (predicted == labels).sum().item() and total += labels.size(0) count correct predictions and total samples. • Finally, accuracy = correct / total and avg_loss = total_loss / len(data_loader) are returned. • device = torch.device (‘cuda’ if torch.cuda.is_available() else ‘cpu’) Selects CUDA if a GPU is available, otherwise defaults to CPU. All model parameters and tensors should be moved to this device for computation.
• model = SimilarityModel(EmbeddingNet()).to(device) Instantiates the Siamese similarity model with the embedding network backbone, and transfers it to the chosen device. • criterion = nn.BCEWithLogitsLoss() Binary cross-entropy with logits combines a sigmoid layer and the BCE loss in a numerically stable single function, suitable for our 0/1 similarity logits. • optimizer = optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-5) Uses the Adam optimizer with a learning rate of 0.01 and L2 weight decay (1e-5) to help regularize the model weights. • scheduler = optim.lr_scheduler.ReduceLROnPlateau (optimizer, patience=2, factor=0.5) Monitors the
validation loss and reduces the learning rate by a factor of 0.5 if no improvement occurs for 2 consecutive epochs,
helping the training escape plateaus. • train_loader = DataLoader(...): Creates a DataLoader over PairDataset with: – batch_size=256: processes 256 pairs at once.
– shuffle=True: randomizes order each epoch.
– num_workers=4: uses 4 subprocesses to load data in parallel.
– pin_memory=True: allocates page-locked memory for faster GPU transfer.
– persistent_workers=True: keeps worker processes alive between epochs to avoid startup overhead. • test_loader = DataLoader(...): Similar setup for the test set, but with shuffle=False so examples are processed in a fixed order. Code 19. DataLoader creation PairDataset(train_dataset, num_pairs=20000), batch_size=256,
shuffle=True, num_workers=4, pin_memory=True, persistent_workers=True, 34
5) test_loader = DataLoader( PairDataset(test_dataset, num_pairs=5000), batch_size=256,
shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, 67 89 Explanation: 6. Data Loading and Training Loop 1 # Create data loaders
2 train_loader = DataLoader( Explanation: • num_epochs = 30: Sets the number of training epochs. Code 20. Training loop setup # Training loop
3 num_epochs = 30
4 print(f"Training on {device}") 12 • for batch_idx, (img1, img2, labels) in enumerate(train_loader): Iterates over batches of paired images and labels. – (Code omitted here for brevity; see full listing below.) optimizer.zero_grad(set_to_none=True) # More efficient than zero_grad() # Use mixed precision for faster training
outputs = model(img1, img2)
loss = criterion(outputs, labels)
loss.backward()
optimizer.step() total_loss += loss.item() if batch_idx % 20 == 0: print(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item() ↪:.4f}") # Evaluate on test set1 23 45 67 89 10 11 12 13 14 Code 22. Evaluation, logging, and checkpointing Explanation: • val_loss, val_acc = evaluate(...): After each epoch, runs the evaluate() function on test_loader, returning average loss and accuracy. • Logging and scheduling: model.train()
total_loss = 0.0 for batch_idx, (img1, img2, labels) in enumerate(train_loader): img1 = img1.to(device, non_blocking=True)
img2 = img2.to(device, non_blocking=True)
labels = labels.to(device, non_blocking=True) Code 21. Epoch and batch loop2 3 45 67 8 Problem [B2]: Calculating Similarity Between Images Using a Neural Network • Prints the device being used for training (CPU or GPU). 1 for epoch in range(num_epochs): Explanation: • for epoch in range(num_epochs): Outer loop over epochs. – model.train(): switches BatchNorm and Dropout to training mode.
– total_loss = 0.0: accumulator for the epoch’s training loss. – Prints average training loss (), validation loss and accuracy.
– scheduler.step(val_loss): reduces learning rate by a factor (0.5) if validation loss hasn’t improved for 2 epochs. len(train_loader) total_loss • Checkpointing: – On the first epoch or whenever the validation loss improves, saves the model’s state_dict to “best_model.pt”. • After all epochs, prints “Training complete!” to signal the end of the loop. 1 # Create data loaders
2 train_loader = DataLoader( PairDataset(train_dataset, num_pairs=20000), batch_size=256,
shuffle=True, num_workers=4, pin_memory=True, persistent_workers=True, 34
5) test_loader = DataLoader( 67 Problem [B2]: Calculating Similarity Between Images Using a Neural Network PairDataset(test_dataset, num_pairs=5000), batch_size=256,
shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, 89
10) 11
12 # Training loop
13 num_epochs = 30
14 print(f"Training on {device}") 15 16 for epoch in range(num_epochs): 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 model.train()
total_loss = 0.0 for batch_idx, (img1, img2, labels) in enumerate(train_loader): img1 = img1.to(device, non_blocking=True)
img2 = img2.to(device, non_blocking=True)
labels = labels.to(device, non_blocking=True) optimizer.zero_grad(set_to_none=True) # More efficient than zero_grad() # Use mixed precision for faster training
outputs = model(img1, img2)
loss = criterion(outputs, labels)
loss.backward()
optimizer.step() total_loss += loss.item() if batch_idx % 20 == 0: print(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item() ↪:.4f}") # Evaluate on test set
val_loss, val_acc = evaluate(model, test_loader, device, criterion) # Log results
print(f"Epoch {epoch+1}/{num_epochs}")
print(f"
print(f" Train Loss: {total_loss/len(train_loader):.4f}")
Test Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}") # Update learning rate based on validation loss
scheduler.step(val_loss) # Saving the best model with each epoch
if epoch == 0 or val_loss < best_val_loss: best_val_loss = val_loss
torch.save(model.state_dict(), "best_model.pt")
print("Model saved!") 54 print("Training complete!") Code 23. Full data loading and training loop 3.4. Results and Visualization
After training, the model can be used to compute similarity scores between arbitrary pairs of images. To visualize the results, we
can select random pairs from the test set, display the images, and show their predicted similarity scores. This helps qualitatively
assess whether the model has learned to output how similar a given image is to other images. Problem [B2]: Calculating Similarity Between Images Using a Neural Network Figure 11. Example pairs of images with predicted similarity scores. [B5] Using Game Theory to Analyze Generative Adversarial Networks (GANs) 4. [B5] Using Game Theory to Analyze Generative Adversarial Networks (GANs) 4.1. Introduction
Generative Adversarial Networks (GANs) are made up of two neural networks that compete with each other: • Generator (𝐺): Tries to create fake data that looks real.
• Discriminator (𝐷): Tries to tell apart real data from fake data. This setup, first introduced by [1], is like a game where both sides try to outsmart each other. GANs are used for things like: • Making new images
• Changing the style of pictures
• Creating text 4.2. Game Theory in GANs 4.2.1. How the Game Works
In the GAN "game": • Players: The generator and discriminator
• Strategies: – The generator tries to make data that fools the discriminator.
– The discriminator tries to spot fake data. • Goal: The generator wants the discriminator to be wrong as often as possible, while the discriminator wants to be right. 4.2.2. The GAN Objective (Minimax Game)
The competition between 𝐺 and 𝐷 can be written as a minimax game with the following formula: min𝐺 max𝐷 𝑉(𝐷, 𝐺) = 𝔼𝑥∼𝑝data[log 𝐷(𝑥)] + 𝔼𝑧∼𝑝𝑧 [log(1 − 𝐷(𝐺(𝑧)))] Here, 𝐷(𝑥) is the probability that 𝑥 is real, and 𝐺(𝑧) is the fake data generated from random noise 𝑧. The generator tries to
minimize this value, while the discriminator tries to maximize it. 4.2.3. When Does the Game End?
The best outcome (called Nash equilibrium) is when: • The generator makes data so real that the discriminator can’t tell the difference (it just guesses).
• The discriminator is right half the time (random guessing). In practice, reaching this balance is hard because: • The networks are complex and hard to optimize.
• Training can be unstable and not always converge. 4.3. Training Challenges 4.3.1. Common Problems • Mode collapse: The generator keeps making the same outputs.
• Oscillations: The networks keep changing but don’t settle down.
• Vanishing gradients: The generator stops learning because the discriminator is too good. 4.3.2. Optimization
Training GANs is like finding a balance point between the two networks. Simple optimization methods are often used, but
sometimes more advanced methods help make training more stable. 4.4. Improvements and Variations 4.4.1. Better GAN Designs • Wasserstein GAN: Uses a different way to measure how close the fake data is to real data, making training smoother.
• Conditional GANs: Lets you control what the generator makes by giving it extra information.
• Multi-agent GANs: Uses more than two networks to make the game more interesting. 4.4.2. Training Tricks • Using reinforcement learning for text or other tricky data.
• Training the networks in steps or with changing difficulty to help them learn better. [B5] Using Game Theory to Analyze Generative Adversarial Networks (GANs) 4.5. Applications and Evaluation 4.5.1. How to Measure Success • Inception Score (IS): Checks if the generated data is varied and realistic.
• Fréchet Inception Distance (FID): Measures how close the generated data is to real data. 4.5.2. Where GANs Are Used • Making new images: CycleGAN, StyleGAN
• Generating text: SeqGAN
• Medical images: MedGAN 4.6. Future Directions
Some open questions for GANs and game theory are: • How to make training more stable and reliable.
• How to analyze games with more than two networks.
• How to guarantee that the networks will reach a good balance. [A1]: Comparing Two Evolutionary Algorithms 5. [A1]: Comparing Two Evolutionary Algorithms
This section documents the solution to the problem of comparing two genetic algorithm variants. The implementation is divided
into the following modules: 5.1. Problem Statement
The goal of this problem is to compare two variants of genetic algorithms for optimizing a modified Rastrigin function ina
three-dimensional search space. The Rastrigin function is a well-known multimodal function, and the modifications introduce
asymmetry, distortion, and rotation to make the optimization landscape more challenging. Specifically, the two algorithms
under comparison are: • Standard DE: A classical differential evolution algorithm implementing mutation, crossover, and selection based on standard differential evolution (DE) strategies. • Simplified Deep-DE: An enhanced variant that integrates a surrogate neural network model to guide the mutation
process, leveraging an experience buffer to train the network on successful mutations and thereby adaptively improve the
search process. The comparison focuses on evaluating the algorithms based on several performance metrics: 1. Solution Quality: The best fitness value achieved and the corresponding candidate solution.
2. Convergence Speed: The number of generations required to reach a threshold close to the optimum.
3. Improvement Rate: The rate of improvement in the fitness value over successive generations.
4. Stability: The consistency of the algorithm’s performance, measured by the variance in fitness during the final generations. 5.2. Methodology
To perform the comparative evaluation of the two genetic algorithm variants, the following methodology is employed: 1. Problem and Benchmark Setup • Modified Rastrigin Function: The optimization problem is based on the Rastrigin function. In order to increase the problem complexity, the function is modified to include: – An asymmetry introduced via a fixed shift in the search space.
– A distortion term that incorporates both quadratic and sub-quadratic (power of 1.5) penalties.
– A rotation term that depends on inter-variable interactions. • Search Space: The function is evaluated in a 3D space where each dimension is bounded between -5.12 and 5.12. 2. Algorithmic Variants
• Standard DE: – Population Initialization: A population of candidate solutions is randomly generated within the problem bounds.
– Mutation and Crossover: For each individual, three distinct parents are randomly selected and used to generatea
mutant solution via the differential evolution strategy. A crossover mechanism then combines the mutant with the
current individual. – Selection: The trial solution replaces the current individual if it achieves a better fitness value. • Simplified Deep-DE: – Surrogate Model Integration: A neural network surrogate is employed to predict promising mutations based on the traits of three parent individuals. – Experience Buffer: Successful mutations (that result in improved fitness) are stored in an experience buffer. This buffer is used to train the surrogate model periodically. – Guided Mutation: Once the surrogate model is sufficiently trained, it is used to generate mutation proposals fora subset of the population, enhancing the exploration of the search space. – Population Update: The algorithm follows a similar crossover and selection strategy as the standard DE. However, the mutation is adaptively guided by the surrogate to converge faster towards the optimum. 3. Experimental Setup and Performance Metrics • Parameter Settings: Both algorithms are executed with the same population size, number of generations, and similar configuration parameters (e.g., crossover rate, scale factor) to ensure a fair comparison. • Metrics Collection: During the evolution process, the following data are collected: – Best fitness value and corresponding candidate solution at each generation.
– The average fitness of the population at each generation.
– Snapshots of the population for visualization purposes. [A1]: Comparing Two Evolutionary Algorithms • Visualization: Post-execution, the following visualizations are created to analyze the performance: – A convergence plot that compares the fitness evolution of both algorithms.
– 3D visualizations (saved as GIFs) of the population evolution over generations.
– A final population comparison plot, illustrating the distribution of candidate solutions in 3D space. 5.3. Implementation
The code implementation for the solution is organized into several parts, let’s discuss them one by one. Problem Definition Explanation: [[-5.12, 5.12], [-5.12, 5.12], [-5.12, 5.12]], dtype=torch.float32) def evaluate(self, population): A = 10.0
n = self.dim
base_rastrigin = A * n + \ torch.sum(population**2 - A * torch.cos(2 * torch.pi * population), dim=1) # Add asymmetry with 3D shift
shift = torch.tensor([-1.23, 2.41, 0.85], dtype=torch.float32)
shifted_pop = population - shift # Add distortion in 3D
distortion = torch.sum(0.5 * shifted_pop**2 + 0.5 * torch.abs(shifted_pop)**1.5, dim=1) # Add rotation terms for 3D
x, y, z = population[:, 0], population[:, 1], population[:, 2]
rotation_term = (0.7 * x * y + 0.3 * x**2 * torch.sin(y) + 0.5 * y * z + 0.2 * z**2 * torch.sin(x)) return base_rastrigin + distortion + rotation_term def evaluate_grid(self, x, y, z): A = 10.0
n = self.dim # Base Rastrigin
base_rastrigin = (A * n + x**2 + y**2 + z**2 - A * np.cos(2 * np.pi * x) -
A * np.cos(2 * np.pi * y) -
A * np.cos(2 * np.pi * z)) # Add asymmetry
shift_x, shift_y, shift_z = -1.23, 2.41, 0.85
x_shifted = x - shift_x
y_shifted = y - shift_y
z_shifted = z - shift_z # Add distortion
distortion = (0.5 * x_shifted**2 + 0.5 * y_shifted**2 + 0.5 * z_shifted**2 + 0.5 * np.abs(x_shifted)**1.5 +
0.5 * np.abs(y_shifted)**1.5 +
0.5 * np.abs(z_shifted)**1.5) # Add rotation term
rotation_term = (0.7 * x * y + 0.3 * x**2 * np.sin(y) + 0.5 * y * z + 0.2 * z**2 * np.sin(x)) return base_rastrigin + distortion + rotation_term5 67 89 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 552 1 class RastriginProblem:
def __init__(self):
self.dim = 3
self.bounds = torch.tensor( 34 [A1]: Comparing Two Evolutionary Algorithms Code 24. Rastrigin Problem Definition (problem.py) The code snippet above defines the RastriginProblem class, which encapsulates the modified Rastrigin function used as
the benchmark optimization problem. The class is implemented using both PyTorch and NumPy to support different evaluation
scenarios. Below is a detailed explanation of the code: • Initialization: In the constructor (__init__), the problem dimensionality is set to 3, meaning that the optimization is
performed in a 3-dimensional space. The search space boundaries for each dimension are defined as [−5.12, 5.12] and
stored in a PyTorch tensor. These bounds ensure that candidate solutions are initialized and kept within the typical limits
for the Rastrigin function. • Fitness Evaluation (evaluate method): The evaluate function calculates the fitness (or cost) of a given population of candidate solutions: – Base Rastrigin Function: The base component of the Rastrigin function is computed as 𝐴 × 𝑛 + ∑ ( 𝑥2
𝑖 − 𝐴 cos(2𝜋𝑥𝑖)), where 𝐴 is set to 10 and 𝑛 is the problem dimensionality. This term represents the standard Rastrigin function.
– Asymmetry Term: A fixed 3D shift is introduced by subtracting a constant vector ([-1.23, 2.41, 0.85]) from each candidate solution. This creates an asymmetric search landscape. – Distortion Term: To further complicate the landscape, an additional distortion is applied to the shifted population.
This term adds both a quadratic and a sub-quadratic component (with an exponent of 1.5) to the fitness value,
intensifying the multimodality of the problem. – Rotation Term: Finally, the function incorporates cross-variable interactions by computing terms like 0.7 ⋅ 𝑥 ⋅ 𝑦,
0.3 ⋅ 𝑥2 sin(𝑦), among others. These terms introduce rotational effects that further increase the complexity of the
optimization landscape. The overall fitness is then obtained by summing the base Rastrigin value, the distortion, and the rotation term. • Grid Evaluation (evaluate_grid method): In addition to evaluating a batch of candidate solutions, the evaluate_grid
method is provided to compute the function values over a grid defined by the variables x, y, and z. The method follows
similar steps as the evaluate method but uses NumPy functions. This is useful for generating 3D contour plots or surface
visualizations of the fitness landscape. def standard_de(problem, pop_size=100, generations=50, F=0.5, CR=0.7, save_snapshots=True, snapshot_interval=20): # Initialize population
lower, upper = problem.bounds[:, 0], problem.bounds[:, 1]
population = torch.rand(pop_size, problem.dim) * (upper - lower) + lower # Evaluate fitness
fitness = problem.evaluate(population) best_fitness_history = [fitness.min().item()]
population_snapshots = [] if save_snapshots: population_snapshots.append(population.clone()) # Main evolution loop
for generation in range(generations): for i in range(pop_size): idxs = [idx for idx in range(pop_size) if idx!= i]
a, b, c = np.random.choice(idxs, 3, replace=False) # Mutation
mutant = population[a] + F * (population[b] - population[c])
mutant = torch.clamp(mutant, lower, upper) 34 56 78 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Implementation of the Classic Differential Evolution: 1 import torch
2 import numpy as np [A1]: Comparing Two Evolutionary Algorithms 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # Crossover
crossover_mask = torch.rand(problem.dim) < CR
if not crossover_mask.any(): crossover_mask[np.random.randint(0, problem.dim)] = True trial = torch.where(crossover_mask, mutant, population[i]) # Selection
trial_fitness = problem.evaluate(trial.unsqueeze(0))
if trial_fitness.item() < fitness[i]: population[i] = trial
fitness[i] = trial_fitness.item() best_fitness_history.append(fitness.min().item()) if save_snapshots and (generation % snapshot_interval == 0 or generation == generations - 1): population_snapshots.append(population.clone()) best_idx = fitness.argmin()
best_solution = population[best_idx]
best_fitness = fitness[best_idx] return best_solution, best_fitness, best_fitness_history, population_snapshots Code 25. Differential Evolution Implementation (classic_de.py) The code snippet above implements the Standard Differential Evolution (DE) algorithm for optimizing the modified Rastrigin problem. The algorithm follows these key steps: • Population Initialization: A population of candidate solutions is randomly generated within the problem bounds. This
is done by sampling uniformly from a tensor of size (pop_size, problem.dim) with values in [0, 1], and then scaling
and shifting the samples based on the provided lower and upper bounds for each dimension. • Fitness Evaluation: Once the population is initialized, the fitness of each candidate solution is computed using the evaluate method of the RastriginProblem class. The best fitness value from the initial population is recorded. • Snapshot Storage: If snapshot saving is enabled, an initial copy of the population is stored in a list. Further snapshots are taken at specified intervals (defined by snapshot_interval) during the evolution process. • Main Evolution Loop: The algorithm iterates for a fixed number of generations. In each generation, every individual in the population is updated via the following steps: – Parent Selection: Three random individuals (distinct from the current individual) are chosen from the population. This selection forms the basis for generating a mutant candidate. – Mutation: A mutant solution is created using the differential evolution mutation strategy: mutant = population[𝑎] + 𝐹 × (population[𝑏] − population[𝑐]), where F is the scale factor that controls the amplification of the differential variation. The mutant is then clamped
to ensure it remains within the problem bounds. – Crossover: A crossover operation combines the mutant with the current individual to produce a trial solution. A
binary mask is generated based on the crossover rate CR, and if no component is selected (i.e., if all entries are false),
one randomly chosen dimension is forced to be taken from the mutant. – Selection: The fitness of the trial solution is evaluated. If the trial solution exhibits a better fitness value than the current individual, it replaces the current individual in the population. • Fitness History and Snapshots: At each generation, the best fitness of the population is recorded. Additionally, snapshots of the population are stored periodically (based on the interval) for later visualization. • Final Solution Extraction: Once the evolution process is complete, the candidate with the best (lowest) fitness value is identified and returned along with the fitness history and snapshots. This implementation of Standard DE serves as a baseline for comparison against the Deep-learning Based DE (Deep-DE) algorithm The code snippet above defines the SimpleSurrogateModel class, which inherits from nn.Module. This surrogate model isa
key component of the Deep DE variant, where it is used to generate mutation proposals based on the characteristics of parent
individuals. Although the full explanation of the Deep DE algorithm will be provided later, the purpose of this model within
that context is as follows: • Network Architecture: The model consists of a simple feedforward neural network with three fully-connected layers. Each layer is followed by a ReLU activation function, which introduces non-linearity. In this implementation: – The input is first transformed to a hidden representation of size 128.
– A second layer maintains the 128-dimensional representation.
– A final layer reduces the dimensionality to half (i.e., 64). • Output Head: One linear head is defined on the final hidden layer: 1. Mutation Head: This linear layer (mutation_head) maps the features to an output of dimension equal to the
problem’s dimensionality. Its purpose is to produce mutation proposals that can help guide the candidate solutions
towards promising regions in the search space. • Forward Method: The forward method processes an input tensor x through the shared network layers. Currently, it
returns the output of the mutation_head, which means that during forward propagation the network provides a mutation
proposal. Role in Deep DE: In the context of Deep DE, the surrogate model uses information from several parent candidates to predict beneficial mutations.
By doing so, it adapts the standard differential evolution process with data-driven guidance, potentially leading to faster
convergence and improved solution quality. The network is periodically trained on successful mutations stored in an experience
buffer, thereby enhancing its predictive capability over generations. This design, with separate heads for mutation generation and potential fitness estimation, is modular enough to be extended and refined as needed for further improvements in the Deep DE framework. def __init__(self, input_dim, output_dim): super().__init__()
hidden_dim = 128 # Simple feedforward network
self.network = nn.Sequential( nn.Linear(input_dim, hidden_dim),
nn.ReLU(),
nn.Linear(hidden_dim, hidden_dim),
nn.ReLU(),
nn.Linear(hidden_dim, hidden_dim // 2),
nn.ReLU(),) self.mutation_head = nn.Linear(hidden_dim // 2, output_dim) def forward(self, x): features = self.network(x)
return self.mutation_head(features) Code 26. Base Neural Network for Deep-DE (main.py) 23 4 56 78 9 10 11 12 13 14 15 16 17 18 19 20 [A1]: Comparing Two Evolutionary Algorithms Base Neural Network for Deep-DE: 1 class SurrogateModel(nn.Module): The ExperienceBuffer class is designed to store and manage successful mutation experiences during the optimization
process in the Deep DE algorithm. This stored experience is later used to train the surrogate model, allowing it to learn from
past successful mutations and improve its predictive performance. The key elements of this class are described below: • Buffer Initialization: In the __init__ method, an empty list buffer is created along with a maximum size (max_size,
defaulting to 1000). This maximum size prevents the buffer from growing indefinitely by limiting the number of stored
experiences. • Adding Experiences: The add method appends new experiences to the buffer. Each experience consists of a tuple containing: – The parent candidate solutions.
– The resulting mutation produced by the surrogate model or standard mutation operator.
– The corresponding fitness value of this mutation. If the buffer has reached its maximum capacity, the oldest experience is removed (using pop(0)) before adding the new
one. def __init__(self, max_size=1000): self.buffer = []
self.max_size = max_size def add(self, parents, mutation, fitness):
if len(self.buffer) >= self.max_size: self.buffer.pop(0) # Remove oldest self.buffer.append((parents, mutation, fitness)) def sample(self, batch_size): if len(self.buffer) < batch_size:
batch_size = len(self.buffer) indices = np.random.choice(len(self.buffer), batch_size, replace=False) parents = torch.stack([self.buffer[i][0] for i in indices])
mutations = torch.stack([self.buffer[i][1] for i in indices])
fitnesses = torch.tensor([self.buffer[i][2] for i in indices], dtype=torch.float32) return parents, mutations, fitnesses def __len__(self): return len(self.buffer) Code 27. ExperienceBuffer Class (main.py) 23 4 56 78 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [A1]: Comparing Two Evolutionary Algorithms Making the network learn from experience (Successful mutants): 1 class ExperienceBuffer: • Sampling Experiences: The sample method allows random extraction of a batch of experiences from the buffer for
training purposes. If the buffer contains fewer experiences than the requested batch size, it returns all available experiences.
The method randomly selects indices from the buffer without replacement and stacks the corresponding parent solutions
and mutations into tensors. The fitness values are also collected in a tensor with type float32. • Buffer Length: The __len__ method returns the current number of experiences stored in the buffer, facilitating dynamic checks during training. Role in Deep DE: Within the Deep DE framework, the Experience Buffer acts as a repository for successful mutation operations. By sampling from
this buffer, the surrogate model is trained to predict beneficial mutations, thereby learning an effective mutation strategy that
can enhance the evolutionary process. This mechanism enables the integration of past experiences into the current mutation
operations, potentially leading to faster and more robust convergence. [A1]: Comparing Two Evolutionary Algorithms Full Implementation of Deep DE: 1 def deep_de(problem, pop_size=100, generations=50, scale_factor=0.5, CR=0.7, 23 45 67 89 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 save_snapshots=True, snapshot_interval=20): # Initialize surrogate model
model = SurrogateModel( input_dim=problem.dim * 3, output_dim=problem.dim) optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = optim.lr_scheduler.CosineAnnealingLR(
optimizer, T_max=generations, eta_min=0.0001) # Initialize experience buffer
buffer = ExperienceBuffer() # Initialize population
population = torch.rand(pop_size, problem.dim)
population = population * \ (problem.bounds[:, 1] - problem.bounds[:, 0]) + problem.bounds[:, 0] # Evaluate fitness
fitness = problem.evaluate(population) best_fitness_history = [torch.min(fitness).item()]
population_snapshots = [] if save_snapshots: population_snapshots.append(population.clone().detach()) # Main evolution loop
for generation in range(generations): new_population = population.clone()
new_fitness = fitness.clone() # Train model
if len(buffer) >= 32: train_iterations = 10 for _ in range(train_iterations): optimizer.zero_grad() # Get training batch
parents_batch, mutations_batch, fitness_batch = buffer.sample( batch_size=32) # Train model to predict mutations
pred_mutations = model(parents_batch)
loss = F.mse_loss(pred_mutations, mutations_batch) loss.backward()
optimizer.step() scheduler.step() # Create new generation
for i in range(pop_size): # Select three random individuals different from the current one
idxs = [idx for idx in range(pop_size) if idx!= i]
a, b, c = np.random.choice(idxs, 3, replace=False) # Prepare parents for neural network
parents = torch.cat( [population[a], population[b], population[c]], dim=0) parents = parents.unsqueeze(0) # Add batch dimension # Use neural network if available, otherwise standard DE
if generation > 5 and len(buffer) > 32: [A1]: Comparing Two Evolutionary Algorithms 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 # Neural network guided mutation
with torch.no_grad(): mutant = model(parents).squeeze(0) else: # Standard DE mutation
mutant = population[a] + scale_factor * \ (population[b] - population[c]) # Boundary handling
mutant = torch.clamp( mutant, problem.bounds[:, 0], problem.bounds[:, 1]) # Crossover
crossover_mask = torch.rand(problem.dim) < CR
if not torch.any(crossover_mask): crossover_mask[np.random.randint(0, problem.dim)] = True trial = torch.where(crossover_mask, mutant, population[i]) # Selection
trial_fitness = problem.evaluate(trial.unsqueeze(0))
if trial_fitness.item() < fitness[i]: new_population[i] = trial
new_fitness[i] = trial_fitness.item() # Add successful mutation to experience buffer
buffer.add(parents.squeeze(0), trial, trial_fitness.item()) # Update population
population = new_population
fitness = new_fitness best_fitness_history.append(torch.min(fitness).item()) if save_snapshots and (generation % snapshot_interval == 0 or generation == generations - 1): population_snapshots.append(population.clone().detach()) # Find best solution
best_idx = torch.argmin(fitness)
best_solution = population[best_idx]
best_fitness = fitness[best_idx] return best_solution, best_fitness, best_fitness_history, population_snapshots Code 28. Full Implementation of Deep DE (main.py) The code snippet above implements the Deep-DE algorithm, an enhanced variant of the standard differential evolution (DE)
method. In this approach, a surrogate neural network is integrated into the evolutionary loop to guide mutation operations.
The surrogate model is periodically trained on successful mutations stored in an experience buffer, enabling the algorithm to
learn and propose promising mutations. Key steps in the Deep-DE algorithm are explained below: • Surrogate Model and Optimizer Initialization: – A SimpleSurrogateModel is instantiated with an input dimension equal to three times the problem dimension (to accommodate three parent candidates) and an output dimension equal to the problem dimension. – An Adam optimizer and a cosine annealing learning rate scheduler are initialized for training the surrogate model. • Experience Buffer Setup: An instance of the ExperienceBuffer is created to store successful mutations. These experiences are later used to train the surrogate model. • Population Initialization and Fitness Evaluation: The initial population is randomly generated within the defined
problem bounds, and its fitness is evaluated using the modified Rastrigin function. Snapshots of the population are taken
for visualization purposes if enabled. • Main Evolution Loop: For each generation, the following steps occur: – Surrogate Model Training: If the experience buffer contains at least 32 experiences, the surrogate model is trained
for a fixed number of iterations (e.g., 10). A batch of experiences is sampled, and the model is trained to predict the [A1]: Comparing Two Evolutionary Algorithms corresponding mutations using mean squared error (MSE) loss. The learning rate scheduler is stepped at the end of
training. – Generation of New Candidate Solutions: For every individual in the population: * Three distinct parent individuals are randomly selected.
* The parents are concatenated and passed through the surrogate model (if the generation is greater than5
and the buffer is adequately populated) to produce a guided mutation. Otherwise, a standard DE mutation is
performed using the formula: mutant = population[𝑎] + scale_factor × (population[𝑏] − population[𝑐]) * The mutant is clamped to ensure it remains within the problem bounds.
* A crossover operation is then applied, where a binary mask (based on the crossover rate CR) selects elements from the mutant or the current individual, yielding a trial solution. * The trial solution’s fitness is evaluated, and if it outperforms the current candidate, it replaces the candidate in the new population. Additionally, successful mutations are added to the experience buffer. – Population Update and Snapshot Storage: After processing all individuals, the population and fitness values are updated. The best fitness is recorded, and periodic snapshots of the population are stored for visualization. • Final Output: Once the evolution loop completes, the algorithm identifies and returns the best candidate solution, its fitness, the history of best fitness values over generations, and the saved population snapshots. Summary of what Deep DE is: The Deep-DE algorithm leverages a surrogate model to inform and accelerate the mutation step of the standard differential
evolution process. By learning from previous successful mutations stored in the experience buffer, the surrogate model adapts
to the problem landscape, potentially leading to faster convergence and improved solution quality. This integrative approach
represents a step towards more adaptive and intelligent evolutionary strategies. [A1]: Comparing Two Evolutionary Algorithms 5.4. Results and Visualization
Performance Comparison: Deep-DE vs Standard DE I start the comparison the 2 algorithms with these initial values: 1 torch.manual_seed(42)
2 np.random.seed(33)
3 pop_size = 100
4 generations = 50
5 snapshot_interval = 20 Code 29. Initialization (main.py) Convergence Analysis
To evaluate the efficiency of Deep-DE over the standard DE algorithm, we visualize the convergence behavior over generations.
As shown in Figure 12, Deep-DE not only converges faster but also achieves a slightly better final fitness value. Figure 12. Convergence curve comparing Deep-DE and standard DE. [A1]: Comparing Two Evolutionary Algorithms Results Summary Terminal Output Summary Results Comparison:
Standard DE:
Best solution: [-0.98752314 0.9926366 -0.01803777]
Best fitness: 4.334627628326416 Deep-DE:
Best solution: [-0.991337 1.0074464 0.00959272]
Best fitness: 4.235910415649414 Improvement by Deep-DE: 2.28% Performance Analysis: • Solution Quality: – Standard DE best fitness: 4.334628
– Deep-DE best fitness: 4.235910
– Improvement: 2.28% • Convergence Speed: – Standard DE converged in: 35 generations
– Deep-DE converged in: 19 generations
– Convergence improvement: 45.71% • Improvement Rate: – Relative improvement rate: -1.30%
– (Positive means Deep-DE improved faster) • Solution Stability: – Stability ratio: 1.00
– (Values > 1 indicate more stability in Deep-DE) Population Evolution Snapshots
Figure 13 shows the population snapshots at different generations for both algorithms. Each frame demonstrates how the
population explores and converges in the 3D search space. Note that darker points are better (Lower values = Approaching
global minimum) [A1]: Comparing Two Evolutionary Algorithms (a) DE Gen0 (b) DE Gen 20 (c) DE Gen 40 (d) DE Gen 60 (e) DE Gen 80 (f) Deep-DE Gen0 (g) Deep-DE Gen 20 (h) Deep-DE Gen 40 Figure 13. Population evolution snapshots over generations (from left to right) for standard DE (top row) and Deep DE (bottom row). (i) Deep-DE Gen 60 (j) Deep-DE Gen 80 [A1]: Comparing Two Evolutionary Algorithms Final Convergence in 3D Space
The final visualization in Figure 15 illustrates how both algorithms converge in the search space. Both algorithms are shown to
have done a pretty good job at finding the global minimum of the function, with a slight edge for the Deep-DE Algorithm (about
1.00% better). Figure 14. Final Population of both algorithms. Figure 15. Final 3D landscape comparison between Deep-DE and Standard DE. All of the visualization code can be found in the visuals.py file in the project directory, I couldn’t add it all here because of how much space it’d have taken. [A6]: Applying LLMs to Academic Paper Evaluation 6. [A6]: Applying LLMs to Academic Paper Evaluation
This section documents the implementation of a Large Language Model (LLM) based system for evaluating academic papers on
both Grammar and Reasoning.The solution combines the use of LLMs with a user-friendly web interface. 6.1. Problem Statement
The goal is to develop an automated system for evaluating academic papers based on two primary criteria: • Grammar Analysis: Evaluation of writing quality, identification of grammatical errors, and suggestions for improvement.
• Reasoning Assessment: Analysis of logical flow, argument structure, and identification of potential logical fallacies or unclear reasoning. This meant I had to think about how I should implement the following: 1. PDF Processing: Text extraction and processing from PDF documents.
2. LLM Integration: Feeding the extracted text data to an LLM that is customized for the task without exceeding its tokens limit. 3. Optional: Web Interface: A way to easily interact with the system to make a little more user-friendly. 6.2. Methodology
The system consists of three main components: 1. PDF Processing: The text of each academic paper is extracted using PDFMiner.six, which helps preserve the structure and formatting of the original document. 2. Retrieval-Augmented Generation (RAG): To handle long documents and avoid exceeding the LLM’s token limit, the
text is split into overlapping chunks. Sentence embeddings are used to find the most relevant chunks for each query, so
only the most important parts are sent to the LLM for evaluation. 3. LLM Evaluation and Web Interface: The selected text is sent to the GROQLLM (deepseek-r1-distill-llama-70b)
for grammar and reasoning analysis. A Django-based web interface that allows papers uploading and results viewing. These steps ensure that large papers are evaluated efficiently and accurately, with results presented clearly to the user System Architecture • Backend Framework: Django-based web app
• External API: GROQLLM integration
• RAG Integration: Hugging face’s ‘all-MiniLM-L6-v2’ sentence-transformers model
• Frontend: JavaScript with TailwindCSS [A6]: Applying LLMs to Academic Paper Evaluation 6.3. Implementation
The system is composed of several modular components, each responsible for a key part of the academic paper evaluation
workflow. Below, I discuss what each part of the code does. Code 30. LLM Manager and GROQAPI Integration (ai_model.py) The code above defines the ‘LLM_Manager‘ class, which acts as a wrapper for interacting with the GROQLLMAPI. It
sets up a default model and provides a method to send chat completion requests to the LLM, handling errors by raisinga
custom exception. The ‘get_chat_completion‘ method is the main entry point for sending messages to the LLM, with options for
streaming and model selection. This abstraction makes it easy to swap out the LLM backend or change model parameters in
one place. class LLM_Manager: def __init__(self): self.client = Groq()
self.default_model = "deepseek-r1-distill-llama-70b" def get_chat_completion(self, messages, stream=True, model=None): try: return self.client.chat.completions.create(
model=model or self.default_model,
messages=messages,
temperature=0,
top_p=1,
stream=stream,) except Exception as e: raise AIModelException(f"Error getting completion: {str(e)}") 34 56 7 89 10 11 12 13 14 15 16 17 18 19 20 21 22 class AIModelException(Exception): 23 pass 1. LLM Manager and GROQAPI Integration 1 #ai_model.py
2 from groq import Groq 2. PDF Processing, Text Cleaning, and Retrieval-Augmented Generation (RAG) 1 #pipeline.py
2 from.ai_model import LLM_Manager
3 import re
4 from pdfminer.high_level import extract_text_to_fp
5 from pdfminer.layout import LAParams
6 from io import StringIO
7 import logging
8 from sentence_transformers import SentenceTransformer
9 import faiss
10 import numpy as np
11 import tiktoken 12 13 class RAG: 14 15 16 17 18 19 20 21 22 23 24 25 def __init__(self, embedding_model_name="all-MiniLM-L6-v2"):
self.model = SentenceTransformer(embedding_model_name)
self.index = None
self.chunks = [] def chunk_text(self, text, chunk_size=1000, overlap=200): words = text.split()
chunks = []
for i in range(0, len(words), chunk_size - overlap): chunk = " ".join(words[i:i+chunk_size])
if chunk: chunks.append(chunk) [A6]: Applying LLMs to Academic Paper Evaluation 26 27 28 29 30 31 32 33 34 35 36 37 return chunks def build_index(self, text): self.chunks = self.chunk_text(text)
embeddings = self.model.encode(self.chunks)
self.index = faiss.IndexFlatL2(embeddings.shape[1])
self.index.add(np.array(embeddings).astype(’float32’)) def retrieve(self, query, top_k=5): query_emb = self.model.encode([query])
D, I = self.index.search(np.array(query_emb).astype(’float32’), top_k)
return [self.chunks[i] for i in I[0]] Code 31. Document Processing, Cleaning, and RAG (pipeline.py) This part defines the ‘RAG‘ class, which is responsible for splitting long documents into overlapping chunks, embedding
them using a sentence transformer, and building a FAISS index for fast similarity search. The ‘chunk_text‘ method breaks the
text into manageable pieces with overlap to preserve context. ‘build_index‘ encodes these chunks and adds them to the FAISS
index, enabling efficient retrieval. The ‘retrieve‘ method finds the most relevant chunks for a given query, which is crucial for
keeping LLM input within token limits and providing focused context. The use of FAISS and sentence embeddings is a key part
of the retrieval-augmented generation pipeline. 3. LLMInterface: File Processing, Cleaning, and Context Retrieval
Class Initialization 1 class LLMInterface: 23 45 def __init__(self): self.ai_manager = LLM_Manager()
self.logger = logging.getLogger(__name__)
self.rag = RAG() Code 32. LLMInterface class and __init__ method (pipeline.py) The ‘LLMInterface‘ class ties together the LLM manager, logging, and the RAG retriever. Its constructor initializes these
components, setting up the main interface for document analysis. This design allows the rest of the workflow to access LLM
and retrieval functionality through a single object. PDF Extraction1 23 45 67 89 10 11 12 13 14 15 16 17 18 19 def extract_pdf_content(self, pdf_file): """Extract text from PDF using PDFMiner.six with optimized parameters."""
output = StringIO()
laparams = LAParams(
line_margin=0.5,
word_margin=0.1,
char_margin=2.0,
boxes_flow=0.5,
detect_vertical=True) try: extract_text_to_fp(pdf_file, output, laparams=laparams)
return output.getvalue() except Exception as e: self.logger.error(f"PDF extraction error: {e}")
raise
finally: output.close() Code 33. extract_pdf_content method (pipeline.py) This method extracts text from a PDF file using PDFMiner.six, with custom layout parameters to better preserve the
document’s structure. It writes the extracted text to a string buffer and handles errors gracefully, logging them for debugging.
The use of ‘finally‘ ensures the buffer is always closed, even if an error occurs. Code 34. process_text_content method (pipeline.py) This function cleans and preprocesses the extracted text. It removes the